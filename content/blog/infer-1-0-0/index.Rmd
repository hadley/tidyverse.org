---
output: hugodown::hugo_document

slug: infer-1-0-0
title: infer 1 0 0
date: 2021-06-15
author: Simon Couch
description: >
    The first major release of infer, a package implementing a unified approach
    to statistical inference, is now on CRAN.

photo:
  url: https://twitter.com/graysonwwhite
  author: Grayson White

categories: [package] 
tags: [tidymodels]
---

<!--
TODO:
* [x] Look over / edit the post's title in the yaml
* [x] Edit (or delete) the description; note this appears in the Twitter card
* [x] Pick category and tags (see existing with `hugodown::tidy_show_meta()`)
* [x] Find photo & update yaml metadata
* [x] Create `thumbnail-sq.jpg`; height and width should be equal
* [x] Create `thumbnail-wd.jpg`; width should be >5x height
* [x] `hugodown::use_tidy_thumbnails()`
* [x] Add intro sentence, e.g. the standard tagline for the package
* [x] `usethis::use_tidy_thanks()`
-->

We're super excited announce the release of [infer](https://infer.tidymodels.org/) 1.0.0. infer is a package for statistical inference that implements an expressive statistical grammar that coheres with the tidyverse design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of four main verbs (functions), supplemented with many utilities to visualize and extract value from their outputs.

You can install it from CRAN with:

```{r, eval = FALSE}
install.packages("infer")
```

This release includes a number of major changes and new features. However, the infer package has been on CRAN since 2017, and we haven't written about the package on the tidyverse blog before. Thus, I'll start out by demonstrating the basics of the package. After, I'll highlight some of the more neat features introduced in this version of the package. You can find a full list of changes in version 1.0.0 of the package in the [release notes](https://github.com/tidymodels/infer/releases/tag/v1.0.0).

```{r setup}
library(infer)
```

## Getting to Know infer

Regardless of the hypothesis test in question, an analyst asks the same kind of question when conducting statistical inference: is the effect/difference in the observed data real, or due to random chance? To answer this question, the analyst begins by assuming that the effect in the observed data was simply due to random chance, and calls this assumption the *null hypothesis*. (In reality, they might not believe in the null hypothesis at all---the null hypothesis is in opposition to the *alternate hypothesis*, which supposes that the effect present in the observed data is actually due to the fact that "something is going on.") The analyst then calculates a *test statistic* from the data that describes the observed effect. They can use this test statistic to calculate a *p-value* via juxtaposition with a *null distribution*, giving the probability that the observed data could come about if the null hypothesis were true. If this probability is below some pre-defined *significance level* $\alpha$, then the analyst can reject the null hypothesis.

The workflow of this package is designed around this idea. Starting out with some dataset,

+ `specify()` allows the analyst to specify the variable, or relationship between variables, that they're interested in.
+ `hypothesize()` allows the analyst to declare the null hypothesis.
+ `generate()` allows the analyst to generate data reflecting the null hypothesis.
+ `calculate()` allows the analyst to calculate summary statistics, either from
     * the observed data, to form the observed test statistic.
     * data `generate()`d to reflect the null hypothesis, to form the randomization-based null distribution of test statistics.

As such, the ultimate output of an infer pipeline using these four functions is generally an _observed statistic_ or _null distribution_ of test statistics. These four functions are thus supplemented with many utilities to visualize and extract value from their outputs.

+ `visualize()` plots the _null distribution_ of test statistics
     * `shade_p_value()` situates the observed statistic in the null distribution, shading the region as or more extreme 
     * `shade_confidence_interval()` situates the confidence interval region in the null distribution, shading the region with the bounds
+ `get_p_value` calculates a p-value via the juxtaposition of the test statistic and null distribution
+ `get_confidence_interval` calculates a confidence interval via the juxtaposition of the test statistic and null distribution

Throughout this post, we make use of `gss`, a dataset supplied by `infer` containing a sample of 500 observations of 11 variables from the *General Social Survey*. 

```{r load-gss, warning = FALSE, message = FALSE}
# load in the dataset
data(gss)

# take a look at its structure
str(gss)
```

Each row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See `?gss` for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let's suppose that this dataset is a representative sample of a population we want to learn about: American adults.

### specify(): Specifying Response (and Explanatory) Variables

The `specify` function can be used to specify which of the variables in the dataset you're interested in. If you're only interested in, say, the `age` of the respondents, you might write:

```{r specify-example, warning = FALSE, message = FALSE}
gss %>%
  specify(response = age)
```

On the front-end, the output of `specify` just looks like it selects off the columns in the dataframe that you've specified. Checking the class of this object, though:

```{r specify-one, warning = FALSE, message = FALSE}
gss %>%
  specify(response = age) %>%
  class()
```

We can see that the `infer` class has been appended on top of the dataframe classes--this new class stores some extra metadata.

If you're interested in two variables--`age` and `partyid`, for example--you can `specify` their relationship in one of two (equivalent) ways:

```{r specify-two, warning = FALSE, message = FALSE}
# as a formula
gss %>%
  specify(age ~ partyid)

# with the named arguments
gss %>%
  specify(response = age, explanatory = partyid)
```

If you're doing inference on one proportion or a difference in proportions, you will need to use the `success` argument to specify which level of your `response` variable is a success. For instance, if you're interested in the proportion of the population with a college degree, you might use the following code:

```{r specify-success, warning = FALSE, message = FALSE}
# specifying for inference on proportions
gss %>%
  specify(response = college, success = "degree")
```

### hypothesize(): Declaring the Null Hypothesis

The next step in the `infer` pipeline is often to declare a null hypothesis using `hypothesize()`. The first step is to supply one of "independence" or "point" to the `null` argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to `hypothesize()`:

```{r hypothesize-independence, warning = FALSE, message = FALSE}
gss %>%
  specify(college ~ partyid, success = "degree") %>%
  hypothesize(null = "independence")
```

If you're doing inference on a point estimate, you will also need to provide one of `p` (the true proportion of successes, between 0 and 1), `mu` (the true mean), `med` (the true median), or `sigma` (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:

```{r hypothesize-40-hr-week, warning = FALSE, message = FALSE}
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40)
```

Again, from the front-end, the dataframe outputted from `hypothesize()` looks almost exactly the same as it did when it came out of `specify()`, but `infer` now "knows" your null hypothesis.

### generate(): Generating the Null Distribution

Once we've asserted our null hypothesis using `hypothesize()`, we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the `type` argument:

* `bootstrap`: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.  
* `permute`: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.  
* `simulate`: A value will be sampled from a theoretical distribution with parameters specified in `hypothesize()` for each replicate. (This option is currently only applicable for testing point estimates.)  

Continuing on with our example above, about the average number of hours worked a week, we might write:

```{r generate-point, warning = FALSE, message = FALSE}
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 1000, type = "bootstrap")
```

In the above example, we take 1000 bootstrap samples to form our null distribution.

To generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 1000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:

```{r generate-permute, warning = FALSE, message = FALSE}
gss %>%
  specify(partyid ~ age) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute")
```

### calculate(): Calculating Summary Statistics

Depending on whether you're carrying out computation-based inference or theory-based inference, you will either supply `calculate()` with the output of `generate()` or `hypothesize`, respectively. The function, for one, takes in a `stat` argument, which is currently one of "mean", "median", "sum", "sd", "prop", "count", "diff in means", "diff in medians", "diff in props", "Chisq", "F", "t", "z", "slope", or "correlation". For example, continuing our example above to calculate the null distribution of mean hours worked per week:

```{r calculate-point, warning = FALSE, message = FALSE}
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")
```

The output of `calculate()` here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you're carrying out inference on differences in means, medians, or proportions, or t and z statistics, you will need to supply an `order` argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don't, we might write:

```{r specify-diff-in-means, warning = FALSE, message = FALSE}
gss %>%
  specify(age ~ college) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate("diff in means", order = c("degree", "no degree"))
```

### Other Utilities

`infer` also offers several utilities to extract the meaning out of summary statistics and null distributions---the package provides functions to visualize where a statistic is relative to a distribution (with `visualize()`), calculate p-values (with `get_p_value()`), and calculate confidence intervals (with `get_confidence_interval()`).

To illustrate, we'll go back to the example of determining whether the mean number of hours worked per week is 40 hours.

```{r utilities-examples}
# find the point estimate
point_estimate <- gss %>%
  specify(response = hours) %>%
  calculate(stat = "mean")

# generate a null distribution
null_dist <- gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")
```

Our point estimate `r point_estimate` seems *pretty* close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn't 40.

We could initially just visualize the null distribution.

```{r visualize, warning = FALSE, message = FALSE}
null_dist %>%
  visualize()
```

Where does our sample's observed statistic lie on this distribution? We can use the `obs_stat` argument to specify this.

```{r visualize2, warning = FALSE, message = FALSE}
null_dist %>%
  visualize() +
  shade_p_value(obs_stat = point_estimate, direction = "two-sided")
```

Notice that `infer` has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the `+` operator to apply the `shade_p_value` function. This is because `visualize` outputs a plot object from `ggplot2` instead of a data frame, and the `+` operator is needed to add the p-value layer to the plot object.) The red bar looks like it's slightly far out on the right tail of the null distribution, so observing a sample mean of `r point_estimate` hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?

```{r get_p_value, warning = FALSE, message = FALSE}
# get a two-tailed p-value
p_value <- null_dist %>%
  get_p_value(obs_stat = point_estimate, direction = "two-sided")

p_value
```

It looks like the p-value is `r p_value`, which is pretty small---if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (`r abs(point_estimate-40)` hours) from 40 would be `r p_value`. This may or may not be statistically significantly different, depending on the significance level $\alpha$ you decided on *before* you ran this analysis. If you had set $\alpha = .05$, then this difference would be statistically significant, but if you had set $\alpha = .01$, then it would not be.

To get a confidence interval around our estimate, we can write:

```{r get_conf, message = FALSE, warning = FALSE}
# start with the null distribution
null_dist %>%
  # calculate the confidence interval around the point estimate
  get_confidence_interval(point_estimate = point_estimate,
                          # at the 95% confidence level
                          level = .95,
                          # using the standard error
                          type = "se")
```

As you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level $\alpha = .05$.

## What's New?

### Support for multiple regression

The 2016 "Guidelines for Assessment and Instruction in Statistics Education" [1] state that, in introductory statistics courses, "[s]tudents should gain experience with how statistical models, including multivariable models, are used." In line with this recommendation, we introduce support for randomization-based inference with multiple explanatory variables via a new `fit.infer` core verb.

If passed an `infer` object, the method will parse a formula out of the `formula` or `response` and `explanatory` arguments, and pass both it and `data` to a `stats::glm` call.

```{r}
gss %>%
  specify(hours ~ age + college) %>%
  fit()
```

Note that the function returns the model coefficients as `estimate` rather than their associated `t`-statistics as `stat`.

If passed a `generate()`d object, the model will be fitted to each replicate.

```{r}
gss %>%
  specify(hours ~ age + college) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  fit()
```

If `type = "permute"`, a set of unquoted column names in the data to permute (independently of each other) can be passed via the `cols` argument to `generate`. It defaults to only the response variable.

```{r}
gss %>%
  specify(hours ~ age + college) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute", cols = c(age, college)) %>%
  fit()
```

This feature allows for more detailed exploration of the effect of disrupting the correlation structure among explanatory variables on outputted model coefficients.

Each of the auxillary functions `get_p_value()`, `get_confidence_interval()`, `visualize()`, `shade_p_value()`, and `shade_confidence_interval()` have methods to handle `fit()` output! See their help-files for example usage.

### Behavioral consistency

Another major change to the package in this release is a set of standards for behavorial consistency of `calculate()`. Namely, the package will now

* supply a consistent error when the supplied `stat` argument isn't well-defined
for the variables `specify()`d

```{r, error = TRUE}
gss %>%
  specify(response = hours) %>%
  calculate(stat = "diff in means")
```

or

```{r, error = TRUE}
gss %>%
  specify(college ~ partyid, success = "degree") %>%
  calculate(stat = "diff in props")
```

* supply a consistent message when the user supplies unneeded information via `hypothesize()` to `calculate()` an observed statistic

```{r}
# supply mu = 40 when it's not needed
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  calculate(stat = "mean")
```

and

* supply a consistent warning and assume a reasonable null value when the user does not supply sufficient information to calculate an observed statistic

```{r}
# don't hypothesize `p` when it's needed
gss %>%
    specify(response = sex, success = "female") %>%
    calculate(stat = "z")
```

or

```{r}
# don't hypothesize `p` when it's needed
gss %>%
  specify(response = partyid) %>%
  calculate(stat = "Chisq")
```

This behavorial consistency also allowed for the implementation of `observe()`, a wrapper function around `specify()`, `hypothesize()`, and `calculate()`, to calculate observed statistics. The function provides a shorthand alternative to calculating observed statistics from data:

```{r}
# calculating the observed mean number of hours worked per week
gss %>%
  observe(hours ~ NULL, stat = "mean")

# equivalently, calculating the same statistic with the core verbs
gss %>%
  specify(response = hours) %>%
  calculate(stat = "mean")

# calculating a t statistic for hypothesized mu = 40 hours worked/week
gss %>%
  observe(hours ~ NULL, stat = "t", null = "point", mu = 40)

# equivalently, calculating the same statistic with the core verbs
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  calculate(stat = "t")
```

We don't anticipate that any of these changes are "breaking" in the sense that code that previously worked will continue to, though it may now message or warn in a way that it did not used to or error with a different (and hopefully more informative) message.

## Acknowledgements

This release was made possible with financial support from RStudio and the Reed College Mathematics Department. Thanks to [&#x0040;aarora79](https://github.com/aarora79), [&#x0040;acpguedes](https://github.com/acpguedes), [&#x0040;AlbertRapp](https://github.com/AlbertRapp), [&#x0040;alexpghayes](https://github.com/alexpghayes), [&#x0040;aloy](https://github.com/aloy), [&#x0040;AmeliaMN](https://github.com/AmeliaMN), [&#x0040;andrewpbray](https://github.com/andrewpbray), [&#x0040;apreshill](https://github.com/apreshill), [&#x0040;atheobold](https://github.com/atheobold), [&#x0040;beanumber](https://github.com/beanumber), [&#x0040;bigdataman2015](https://github.com/bigdataman2015), [&#x0040;bragks](https://github.com/bragks), [&#x0040;brendanhcullen](https://github.com/brendanhcullen), [&#x0040;CarlssonLeo](https://github.com/CarlssonLeo), [&#x0040;ChalkboardSonata](https://github.com/ChalkboardSonata), [&#x0040;chriscardillo](https://github.com/chriscardillo), [&#x0040;clauswilke](https://github.com/clauswilke), [&#x0040;congdanh8391](https://github.com/congdanh8391), [&#x0040;corinne-riddell](https://github.com/corinne-riddell), [&#x0040;cristianvaldez](https://github.com/cristianvaldez), [&#x0040;daranzolin](https://github.com/daranzolin), [&#x0040;davidbaniadam](https://github.com/davidbaniadam), [&#x0040;davidhodge931](https://github.com/davidhodge931), [&#x0040;doug-friedman](https://github.com/doug-friedman), [&#x0040;dshelldhillon](https://github.com/dshelldhillon), [&#x0040;dsolito](https://github.com/dsolito), [&#x0040;echasnovski](https://github.com/echasnovski), [&#x0040;EllaKaye](https://github.com/EllaKaye), [&#x0040;enricochavez](https://github.com/enricochavez), [&#x0040;gdbassett](https://github.com/gdbassett), [&#x0040;ghost](https://github.com/ghost), [&#x0040;GitHunter0](https://github.com/GitHunter0), [&#x0040;hardin47](https://github.com/hardin47), [&#x0040;hfrick](https://github.com/hfrick), [&#x0040;higgi13425](https://github.com/higgi13425), [&#x0040;instantkaffee](https://github.com/instantkaffee), [&#x0040;ismayc](https://github.com/ismayc), [&#x0040;jbourak](https://github.com/jbourak), [&#x0040;jcvall](https://github.com/jcvall), [&#x0040;jimrothstein](https://github.com/jimrothstein), [&#x0040;kennethban](https://github.com/kennethban), [&#x0040;m-berkes](https://github.com/m-berkes), [&#x0040;mikelove](https://github.com/mikelove), [&#x0040;mine-cetinkaya-rundel](https://github.com/mine-cetinkaya-rundel), [&#x0040;Minhasshazu](https://github.com/Minhasshazu), [&#x0040;msberends](https://github.com/msberends), [&#x0040;mt-edwards](https://github.com/mt-edwards), [&#x0040;muschellij2](https://github.com/muschellij2), [&#x0040;nfultz](https://github.com/nfultz), [&#x0040;nicholasjhorton](https://github.com/nicholasjhorton), [&#x0040;PirateGrunt](https://github.com/PirateGrunt), [&#x0040;PsychlytxTD](https://github.com/PsychlytxTD), [&#x0040;richierocks](https://github.com/richierocks), [&#x0040;romainfrancois](https://github.com/romainfrancois), [&#x0040;rpruim](https://github.com/rpruim), [&#x0040;rudeboybert](https://github.com/rudeboybert), [&#x0040;rundel](https://github.com/rundel), [&#x0040;sastoudt](https://github.com/sastoudt), [&#x0040;sbibauw](https://github.com/sbibauw), [&#x0040;sckott](https://github.com/sckott), [&#x0040;simonpcouch](https://github.com/simonpcouch), [&#x0040;THargreaves](https://github.com/THargreaves), [&#x0040;topepo](https://github.com/topepo), [&#x0040;torockel](https://github.com/torockel), [&#x0040;ttimbers](https://github.com/ttimbers), [&#x0040;vikram-rawat](https://github.com/vikram-rawat), [&#x0040;vladimirvrabely](https://github.com/vladimirvrabely), and [&#x0040;xiaochi-liu](https://github.com/xiaochi-liu) for their contributions to the package.
