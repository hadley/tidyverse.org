---
title: bench 1.0.1
date: '2018-06-28'
slug: bench-1.0.1
author: Jim Hester
categories: [package]
description: >
  bench 1.0.1 is now available on CRAN. bench allows you to benchmark
  code, by tracking execution time, memory allocations and garbage
  collections.
photo:
  url: https://unsplash.com/photos/nIkuMWT4Imc
  author: Rula Sibai
---



[bench] is now available on CRAN!

The goal of [bench] is to benchmark code, by tracking execution time,
memory allocations and garbage collections.

Install the latest version with:
``` r
install.packages("bench")
```

### Usage

Benchmarks can be run with `bench::mark()`, which takes one or more expressions
to benchmark against each other.


```r
library(bench)
set.seed(42)
dat <- data.frame(x = runif(10000, 1, 1000), y=runif(10000, 1, 1000))
```

`bench::mark()` will throw an error if the results are not equivalent, so you
don't accidentally benchmark non-equivalent code.

```r
bench::mark(
  dat[dat$x > 500, ],
  dat[which(dat$x > 499), ],
  subset(dat, x > 500))
#> Error: Each result must equal the first result:
#>   `dat[dat$x > 500, ]` does not equal `dat[which(dat$x > 499), ]`
```

Results are easy to interpret, with human readable units in a rectangular data frame.

```r
bnch <- bench::mark(
  dat[dat$x > 500, ],
  dat[which(dat$x > 500), ],
  subset(dat, x > 500))
bnch
#> # A tibble: 3 x 10
#>   expression                     min     mean   median      max `itr/sec` mem_alloc  n_gc n_itr total_time
#>   <chr>                     <bch:tm> <bch:tm> <bch:tm> <bch:tm>     <dbl> <bch:byt> <dbl> <int>   <bch:tm>
#> 1 dat[dat$x > 500, ]           284µs    326µs    302µs 893.78µs     3064.     416KB    97  1097      358ms
#> 2 dat[which(dat$x > 500), ]    212µs    257µs    234µs   1.18ms     3888.     357KB    91  1301      335ms
#> 3 subset(dat, x > 500)         345µs    373µs    359µs 910.37µs     2683.     548KB   101   849      316ms
```

By default, the summary uses absolute measures, however relative results can be
obtained by using `relative = TRUE` in your call to `bench::mark()` or by calling
`summary(relative = TRUE)` on the results.

```r
summary(bnch, relative = TRUE)
#> # A tibble: 3 x 10
#>   expression                  min  mean median   max `itr/sec` mem_alloc  n_gc n_itr total_time
#>   <chr>                     <dbl> <dbl>  <dbl> <dbl>     <dbl>     <dbl> <dbl> <dbl>      <dbl>
#> 1 dat[dat$x > 500, ]         1.34  1.27   1.29  1         1.14      1.16  1.07  1.29       1.13
#> 2 dat[which(dat$x > 500), ]  1     1      1     1.32      1.45      1     1     1.53       1.06
#> 3 subset(dat, x > 500)       1.62  1.45   1.54  1.02      1         1.53  1.11  1          1
```

`bench::press()` is used to run benchmarks against a grid of parameters.
Provide setup and benchmarking code as a single unnamed argument then define
sets of values as named arguments. The full combination of values will be
expanded and the benchmarks are then _pressed_ together in the result. This
allows you to benchmark a set of expressions across a wide variety of input
sizes, perform replications and other useful tasks.


```r
set.seed(42)

create_df <- function(rows, cols) {
  as.data.frame(setNames(
    replicate(cols, runif(rows, 1, 1000), simplify = FALSE),
    rep_len(c("x", letters), cols)))
}

results <- bench::press(
  rows = c(10000, 100000),
  cols = c(10, 100),
  {
    dat <- create_df(rows, cols)
    bench::mark(
      min_iterations = 100,
      bracket = dat[dat$x > 500, ],
      which = dat[which(dat$x > 500), ],
      subset = subset(dat, x > 500)
    )
  }
)
results
#> # A tibble: 12 x 12
#>    expression   rows  cols      min     mean   median      max `itr/sec` mem_alloc  n_gc n_itr total_time
#>    <chr>       <dbl> <dbl> <bch:tm> <bch:tm> <bch:tm> <bch:tm>     <dbl> <bch:byt> <dbl> <int>   <bch:tm>
#>  1 bracket     10000    10 719.72µs 820.85µs 767.54µs   2.19ms   1218.      1.17MB    84   293   240.51ms
#>  2 which       10000    10 389.07µs 434.84µs  413.9µs   1.35ms   2300.    827.04KB   104   597    259.6ms
#>  3 subset      10000    10 786.42µs 905.91µs 848.75µs   1.97ms   1104.      1.28MB    58   295   267.24ms
#>  4 bracket    100000    10  12.81ms  14.61ms  14.52ms     19ms     68.4    11.54MB    62    39   569.94ms
#>  5 which      100000    10    8.9ms  10.41ms  10.39ms  12.79ms     96.1     7.91MB    42    58   603.83ms
#>  6 subset     100000    10  14.07ms  16.01ms  15.98ms   20.3ms     62.5    12.68MB    85    28    448.2ms
#>  7 bracket     10000   100   5.61ms   6.96ms   6.73ms  10.54ms    144.      9.71MB    43    57   396.63ms
#>  8 which       10000   100   2.51ms   3.27ms   3.26ms   5.34ms    306.      5.91MB    31    87   284.69ms
#>  9 subset      10000   100   5.96ms      7ms    6.9ms   9.06ms    143.      9.84MB    56    45   314.83ms
#> 10 bracket    100000   100   97.5ms 101.01ms 100.67ms 108.74ms      9.90   97.47MB    94    14      1.41s
#> 11 which      100000   100  54.52ms  56.77ms  55.94ms  66.16ms     17.6    59.51MB    56    44       2.5s
#> 12 subset     100000   100  98.23ms 102.08ms 101.13ms 110.84ms      9.80   98.62MB    92    23      2.35s
```

### Plotting

`ggplot2::autoplot()` can be used to generate an informative default plot. This
plot is colored by GC level (0, 1, or 2) and faceted by parameters (if any). By
default it generates a
[beeswarm](https://github.com/eclarke/ggbeeswarm#geom_quasirandom) plot,
however you can also specify other plot types (`jitter`, `ridge`,
`boxplot`, `violin`). See `?autoplot.bench_mark` for full details. This gives
you a nice overview of the runs and allows you to gauge the effects of garbage
collection on the results.


```r
ggplot2::autoplot(results)
```

<img src="/articles/2018-06-bench-1.0.1_files/figure-html/autoplot-1.png" width="100%" />

You can also produce fully custom plots by un-nesting the results and
working with the data directly. In this case we are exploring how the amount of
memory allocated by each expression interacts with the time taken to run.


```r
library(tidyverse)
results %>%
  unnest() %>%
  filter(gc == "none") %>%
  ggplot(aes(x = mem_alloc, y = time, color = expression)) +
    geom_point() +
    scale_color_brewer(type = "qual", palette = 3) +
    geom_smooth(method = "lm", se = F, colour = "grey50")
```

<img src="/articles/2018-06-bench-1.0.1_files/figure-html/custom-plot-1.png" width="100%" />

### Compared to existing methods

Compared to other methods such as [system.time], [rbenchmark], [tictoc] or
[microbenchmark] we feel it has a number of benefits.

- Uses the highest precision APIs available for each operating system (often nanosecond-level).
- Tracks memory allocations for each expression.
- Tracks the number and type of R garbage collections per run.
- Verifies equality of expression results by default, to avoid accidentally
  benchmarking non-equivalent code.
- Uses adaptive stopping by default, running each expression for a set amount
  of time rather than for a specific number of iterations.
- Runs expressions in batches and calculates summary statistics after
  filtering out iterations with garbage collections. This allows you to isolate
  the performance and effects of garbage collection on running time (for more details see [Neal
  2014](https://radfordneal.https://dynverse.github.io/dynverse/wordpress.com/2014/02/02/inaccurate-results-from-microbenchmark/)).
- Allows benchmarking across a grid of input values with `bench::press()`.

### Dependency load

When the development version of **bench** was
[introduced](https://twitter.com/jimhester_/status/996063591433416704) a few
people expressed concern over the number of dependencies in the package. I will
attempt to explain why these dependencies exist and why the true load may actually
be less than you might think.

While bench currently has 19 dependencies, only 8 of these are hard
dependencies; that is they are needed to install the package. Of these 8 hard
dependencies 3 of them (methods, stats, utils) are base packages installed with
R. Of these 5 remaining packages 3 have no additional dependencies (glue,
profmem, rlang). The two remaining packages (tibble and pillar) are used to
provide nice printing of the times and memory sizes and support for list
columns to store the timings, garbage collections, and allocations. These are
major features of the bench package and it would not work without these
dependencies.

The remaining 11 packages are soft dependencies, used either for testing or for
optional functionality, most notably plotting. They will not be installed
unless explicitly requested.

The [microbenchmark] package is a good alternative for those looking for a
package with only base dependencies.

### Feedback wanted!

We hope **bench** is a useful tool for benchmarking short expressions of code.
Please open [GitHub issues](https://github.com/r-lib/bench) for any feature
requests or bugs.

Learn more about **bench** at http://bench.r-lib.org

A big thanks goes to all the community members who contributed code and opened
issues since for this release!
[\@espinielli](https://github.com/espinielli),
[\@hadley](https://github.com/hadley),
[\@HughParsonage](https://github.com/HughParsonage),
[\@jasonserviss](https://github.com/jasonserviss),
[\@jimhester](https://github.com/jimhester),
[\@jonocarroll](https://github.com/jonocarroll),
[\@lionel-](https://github.com/lionel-),
[\@MilesMcBain](https://github.com/MilesMcBain),
[\@njtierney](https://github.com/njtierney), and
[\@zkamvar](https://github.com/zkamvar)

[bench]: https://bench.r-lib.org
[rbenchmark]: https://cran.r-project.org/package=rbenchmark
[tictoc]: https://cran.r-project.org/package=tictoc
[microbenchmark]: https://cran.r-project.org/package=microbenchmark
[system.time]: https://www.rdocumentation.org/packages/base/versions/3.5.0/topics/system.time
